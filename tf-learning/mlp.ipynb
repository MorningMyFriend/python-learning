{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda382e4dcbe19848d9b0183c376ab62036",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "load done： (60000, 28, 28) (60000,)\n"
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# 数据 28*28*1 的图像\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = tf.cast(x_train, tf.float32) / 255.0\n",
    "x_test = tf.cast(x_test, tf.float32) / 255.0\n",
    "train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# 参数\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "W1 = tf.Variable(tf.random.normal(shape=(num_inputs, num_hiddens), mean=0, stddev=0.01, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.zeros(num_hiddens, dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.random.normal(shape=(num_hiddens,num_outputs), mean=0, stddev=0.01, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.random.normal([num_outputs], stddev=0.1))\n",
    "\n",
    "print('load done：', x_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4030604e-03\n 5.55768097e-03 5.75951457e-01 2.99118254e-02 7.09272802e-01\n 3.24828625e-01 3.50896060e-03 6.48542523e-01 7.30391731e-03], shape=(128,), dtype=float32)\nnet(x) outputs y: (128, 10)    tf.Tensor(\n[[2.8851991e-07 5.6832961e-09 2.7566530e-08 ... 7.8063029e-05\n  3.3706394e-06 9.7136569e-01]\n [4.1024942e-02 1.2121729e-03 2.9723952e-03 ... 4.9993596e-06\n  1.3042666e-04 4.3354407e-06]\n [5.9939486e-01 1.5169961e-02 5.2735154e-02 ... 3.4142277e-04\n  2.2538032e-03 2.8358115e-04]\n ...\n [4.8361280e-06 9.9982351e-01 7.4601430e-06 ... 9.2888406e-08\n  2.0116634e-08 3.7191967e-08]\n [3.8672462e-02 4.2982292e-01 3.2482073e-02 ... 2.9291632e-03\n  5.1206970e-03 2.9679891e-03]\n [8.1647646e-05 5.8280671e-06 3.3969132e-05 ... 3.8116113e-03\n  1.5117563e-03 5.5219233e-02]], shape=(128, 10), dtype=float32)\ny_true: (128,)    tf.Tensor(\n[9 3 0 1 5 4 9 5 9 7 6 0 1 7 8 7 0 1 9 9 7 2 8 8 5 6 0 0 6 4 5 8 7 6 8 9 7\n 2 7 1 7 3 5 1 2 1 7 5 4 8 3 0 9 9 8 1 1 5 7 8 7 7 8 2 0 8 3 9 1 1 0 1 7 1\n 7 7 7 1 1 2 7 7 8 4 2 9 3 4 2 7 2 7 5 3 3 0 5 3 9 8 4 5 9 4 6 2 2 2 5 2 6\n 4 2 6 4 9 3 5 8 2 4 3 3 9 0 1 3 5], shape=(128,), dtype=uint8)\nloss outputs: tf.Tensor(\n[2.90526431e-02 6.28978387e-02 5.11834741e-01 8.58284719e-03\n 2.54450087e-03 1.74503863e-01 4.66833793e-04 1.44830868e-02\n 7.55512528e-03 2.30778918e-01 1.77850842e+00 2.43128926e-01\n 1.78511452e-03 1.69964377e-02 5.39252907e-03 1.02835882e+00\n 1.26835227e-01 1.58148315e-02 7.66245872e-02 1.03725400e-02\n 5.66003621e-01 2.37548742e-02 1.28020323e-03 3.88201582e-03\n 2.36477703e-02 1.20238161e+00 3.22397016e-02 2.29298085e-01\n 5.80396473e-01 1.19886827e+00 9.62899476e-02 3.97712784e-03\n 3.67159806e-02 1.03508338e-01 3.27477977e-02 2.94095781e-02\n 9.58682671e-02 1.21744788e+00 1.33772635e+00 2.73477286e-01\n 4.39374536e-01 3.64579936e-03 3.27607207e-02 1.82033691e-03\n 2.57812794e-02 3.11131407e-05 3.10163140e-01 1.44573553e-02\n 7.98748553e-01 2.23719981e-03 1.00215650e+00 3.43417302e-02\n 1.50032211e-02 7.89743513e-02 3.40964599e-03 7.89093554e-01\n 3.00958171e-04 2.24346016e-02 5.21086622e-03 5.55590261e-03\n 8.94515309e-03 9.25671235e-02 2.43367925e-01 3.99439245e-01\n 1.90212831e-01 6.65369406e-02 2.64038086e+00 6.69225864e-03\n 3.26398225e-03 2.00251554e-04 2.11724922e-01 1.31287435e-02\n 6.80534616e-02 4.95549408e-04 1.85671821e-01 3.00439924e-01\n 1.25866818e+00 1.91431798e-04 3.68050562e-04 3.32140654e-01\n 1.12180851e-01 2.93281991e-02 2.48279870e-01 1.00202751e+00\n 2.11978507e+00 9.73263919e-01 1.64894182e-02 4.37231481e-01\n 1.78776309e-02 7.04696476e-01 1.91531740e-02 8.93783420e-02\n 5.79000916e-03 5.86391330e-01 7.36258984e-01 3.31438966e-02\n 1.33162662e-02 2.24968553e-01 1.96248312e-02 3.02901976e-02\n 4.69654232e-01 5.36317863e-02 1.84297077e-02 1.33436933e-01\n 1.03166294e+00 4.75067258e-01 5.80150902e-01 6.10001802e-01\n 2.29726359e-02 5.11153162e-01 3.12546700e-01 5.86812735e-01\n 6.53969571e-02 1.45376846e-01 1.23266861e-01 1.38613120e-01\n 9.22063645e-03 2.48235371e-02 2.24029226e-03 1.27267838e+00\n 1.51207280e+00 1.28209150e+00 1.39133608e+00 5.59632713e-03\n 3.16007853e-01 1.77010123e-04 1.08351350e+00 6.27528653e-02], shape=(128,), dtype=float32)\nnet(x) outputs y: (96, 10)    tf.Tensor(\n[[7.37877548e-01 2.44327332e-03 5.70823587e-02 1.39136007e-02\n  8.88573565e-03 7.33461638e-05 1.70137614e-01 4.19552089e-06\n  9.57488362e-03 7.36537095e-06]\n [3.37201655e-01 7.76147377e-03 1.98351860e-01 5.19902892e-02\n  1.52792446e-02 3.46191810e-03 2.76594341e-01 3.65738408e-04\n  1.08592890e-01 4.00623423e-04]\n [8.36434662e-01 7.53523418e-05 3.19505995e-03 8.77841655e-03\n  1.35994153e-04 8.73170848e-07 1.51233867e-01 7.43467183e-07\n  1.42680583e-04 2.39203996e-06]\n [6.82845784e-05 3.80844256e-07 2.21428527e-05 1.49343296e-05\n  1.06551532e-07 2.90871143e-01 1.33664689e-05 7.11071212e-03\n  6.61724538e-04 7.01237202e-01]\n [7.69142527e-03 4.24649660e-03 7.09462941e-01 1.31970905e-02\n  6.99731335e-02 1.68063806e-03 1.90925241e-01 5.15141983e-05\n  2.69578141e-03 7.57022062e-05]\n [8.98446970e-08 4.05384304e-09 1.97473309e-08 9.11021516e-08\n  7.36451433e-09 9.32698837e-04 3.10523980e-08 1.47213504e-01\n  4.90296661e-05 8.51804495e-01]\n [5.60581982e-02 6.28413856e-02 2.54507195e-02 8.12705576e-01\n  7.71582173e-03 3.61112179e-03 2.19144821e-02 4.85704839e-03\n  2.34500342e-03 2.50058435e-03]\n [4.29914035e-02 1.25054491e-03 1.04682427e-03 9.50337589e-01\n  4.89581071e-05 2.20698610e-07 4.29153955e-03 1.05432355e-05\n  1.06167090e-05 1.17305754e-05]\n [6.84130704e-03 1.01210857e-02 8.48592281e-01 9.02169384e-03\n  4.32009362e-02 2.45546427e-04 8.07086974e-02 1.26150244e-05\n  1.23683864e-03 1.89887014e-05]\n [2.97907777e-02 4.96895693e-04 7.12880418e-02 1.71383738e-03\n  1.64989159e-02 1.08284957e-03 4.52203527e-02 7.18792493e-04\n  8.32525551e-01 6.64094463e-04]\n [7.65946368e-03 9.48125005e-01 5.26273390e-03 2.66918074e-02\n  1.08970487e-02 1.09601919e-04 6.89288951e-04 3.07011185e-04\n  1.93206200e-04 6.48986024e-05]\n [1.00872666e-03 9.95167494e-01 3.41509731e-04 2.87264399e-03\n  5.55693230e-04 8.05120999e-06 1.21414150e-05 2.24816795e-05\n  8.36945856e-06 2.70349460e-06]\n [4.30568215e-03 9.82372556e-04 6.98699290e-03 8.65772516e-02\n  8.46397519e-01 6.30555462e-07 5.06352596e-02 2.36772303e-06\n  4.11162060e-03 2.96311185e-07]\n [1.56537455e-03 2.45674019e-04 5.22002816e-01 3.53257637e-03\n  2.15584651e-01 6.82975997e-06 2.54978538e-01 2.99211865e-06\n  2.07887404e-03 1.62532751e-06]\n [8.88773957e-06 9.99961495e-01 2.93939729e-06 1.09665443e-05\n  1.55681810e-05 2.11959961e-09 1.04705133e-08 2.44909124e-08\n  6.18245366e-09 1.23925536e-09]\n [1.72839928e-02 8.51269986e-04 1.60771273e-02 1.56182749e-03\n  9.46808606e-03 7.55594485e-03 2.06390172e-02 1.81933620e-03\n  9.24012721e-01 7.30633910e-04]\n [1.87967134e-06 2.62906710e-08 2.44727516e-07 4.75159680e-07\n  6.58434658e-08 4.27858578e-03 5.97396763e-07 3.84149700e-02\n  1.70661311e-04 9.57132518e-01]\n [1.43398584e-05 1.64247525e-04 4.32831794e-02 3.55986776e-05\n  9.46040988e-01 8.22385346e-07 1.02528743e-02 3.46840090e-09\n  2.07994963e-04 4.31081162e-08]\n [9.09843326e-01 9.65218351e-05 5.65412873e-03 1.23703340e-03\n  1.26325613e-04 8.35075440e-08 8.28238055e-02 6.76184726e-08\n  2.18474743e-04 2.04311462e-07]\n [1.41613884e-02 7.08329753e-05 2.80977245e-02 7.05532264e-04\n  2.41552526e-03 1.67012215e-04 1.94203313e-02 5.64751790e-05\n  9.34674442e-01 2.30798643e-04]\n [9.55039203e-01 8.45113024e-03 4.15359763e-03 1.33448513e-02\n  1.71839842e-04 1.62225933e-05 1.87265445e-02 4.02048499e-06\n  8.55154722e-05 7.14584485e-06]\n [5.35659183e-06 9.98810053e-01 1.39922995e-05 1.09238154e-03\n  7.79970287e-05 9.68716929e-09 4.47410180e-08 1.12778359e-07\n  1.37946339e-08 6.33307735e-08]\n [1.05589919e-01 1.52585208e-01 6.86258376e-02 3.98693770e-01\n  7.03634620e-02 3.82633582e-02 1.32574007e-01 7.67978979e-03\n  7.24271638e-03 1.83819719e-02]\n [1.82688473e-05 1.26767582e-05 9.56116128e-05 1.53007102e-04\n  5.36420521e-05 9.34526503e-01 4.88755213e-05 5.41392937e-02\n  4.39610286e-03 6.55594841e-03]\n [7.41275102e-02 9.63248406e-03 5.70055237e-03 8.86916518e-01\n  1.78526877e-03 8.97469477e-07 2.16112230e-02 3.44782929e-05\n  4.55494301e-05 1.45546757e-04]\n [2.64725299e-04 1.97874598e-07 3.01197957e-04 2.02790420e-06\n  3.80759120e-05 5.78115511e-08 1.35368042e-04 2.62650633e-06\n  9.99248207e-01 7.46080104e-06]\n [2.03996624e-05 5.45574585e-05 2.38025680e-01 7.68768077e-05\n  7.50075042e-01 5.49269394e-08 1.13668041e-02 8.59338212e-10\n  3.80583602e-04 1.56763080e-09]\n [2.98622646e-04 9.97517109e-01 1.38646399e-04 1.71037286e-03\n  3.15337966e-04 1.15704711e-06 4.31427952e-06 1.05892314e-05\n  1.57854709e-06 2.35869061e-06]\n [1.49067557e-06 2.25541683e-08 2.45849947e-07 2.76278456e-07\n  9.45708791e-08 2.74014100e-03 6.39302982e-07 2.27052402e-02\n  1.84663077e-04 9.74367201e-01]\n [9.45731699e-01 1.07287422e-04 3.27369221e-03 4.51957621e-03\n  1.42131656e-04 5.13680959e-07 4.60918322e-02 3.45785281e-07\n  1.31736306e-04 1.08469101e-06]\n [7.30767250e-01 6.26879337e-04 2.75761681e-03 4.41211574e-02\n  4.86334180e-03 1.73912241e-07 2.16634333e-01 1.39282065e-07\n  2.28829871e-04 2.23369724e-07]\n [2.00435752e-03 1.95215328e-03 2.63582158e-04 9.95363235e-01\n  1.75367182e-04 1.09477249e-07 2.31497062e-04 2.87486296e-06\n  5.34626815e-06 1.40268992e-06]\n [2.83447080e-05 9.99894261e-01 1.05332501e-05 2.98270079e-05\n  3.69409463e-05 1.54244617e-08 5.18754284e-08 1.07417982e-07\n  1.99158983e-08 9.84097337e-09]\n [4.83594202e-02 2.90887151e-02 7.89196014e-01 1.53855123e-02\n  4.02491838e-02 2.18189904e-03 7.19572306e-02 7.38253293e-05\n  3.42675461e-03 8.15148960e-05]\n [7.83467476e-06 9.99959111e-01 3.77267565e-06 1.29078362e-05\n  1.63400491e-05 1.59973113e-09 1.03939408e-08 3.50051970e-08\n  7.94325494e-09 2.37944531e-09]\n [4.93679045e-06 2.02307447e-07 1.16080537e-06 3.18066895e-06\n  2.28528421e-07 7.81677850e-03 1.33844719e-06 3.33234876e-01\n  5.73464786e-04 6.58363760e-01]\n [2.48019454e-08 8.30411437e-08 9.51309005e-08 6.76073000e-07\n  5.29933821e-08 1.53360458e-03 2.51136001e-08 9.98199821e-01\n  5.18398265e-05 2.13732565e-04]\n [2.29260011e-04 3.27687922e-05 3.36739176e-04 1.67168735e-04\n  6.21228901e-05 9.89080012e-01 4.95463086e-04 5.28184522e-04\n  2.37020757e-03 6.69807196e-03]\n [1.46747602e-03 1.61664400e-04 5.32631397e-01 1.66844460e-03\n  1.60835445e-01 1.05520962e-06 2.59634942e-01 1.34068898e-07\n  4.35974486e-02 2.05772176e-06]\n [9.74823654e-01 2.92790734e-04 2.27289554e-03 3.22780572e-03\n  7.81404378e-05 4.61889783e-07 1.92508250e-02 3.15359557e-07\n  5.07872901e-05 2.33724245e-06]\n [7.50455400e-03 3.80844879e-03 9.13560271e-01 7.97843747e-03\n  2.80306973e-02 2.67610776e-05 3.80609259e-02 3.18875050e-06\n  1.02156913e-03 5.17202898e-06]\n [5.57839929e-04 1.78476083e-04 9.67265368e-01 1.46964507e-04\n  1.30559839e-02 9.41574399e-07 1.86286196e-02 2.28363408e-08\n  1.65578633e-04 8.88846117e-08]\n [1.83191011e-03 9.92411256e-01 8.95666308e-04 3.83218448e-03\n  9.29870526e-04 1.60286836e-05 2.39422334e-05 3.60808808e-05\n  1.85284389e-05 4.47763523e-06]\n [5.00993384e-03 1.26950815e-03 5.68722421e-03 3.91279459e-01\n  5.52255511e-01 2.38111539e-07 4.09932137e-02 4.70108807e-06\n  3.49956308e-03 5.74611363e-07]\n [6.65387046e-03 4.50656171e-06 6.52084313e-03 8.48801399e-04\n  3.97716125e-04 1.36276905e-03 1.23275956e-02 1.12560565e-05\n  9.71650124e-01 2.22511764e-04]\n [1.03287445e-02 1.91358959e-05 6.06298968e-02 1.38655619e-03\n  1.09733641e-03 3.38738801e-07 1.70517825e-02 1.07816481e-06\n  9.09479737e-01 5.43958595e-06]\n [3.36653809e-03 1.81351134e-04 1.02328598e-01 2.16222205e-03\n  1.21788763e-01 1.23922786e-04 7.66618133e-01 4.31102563e-07\n  3.42736416e-03 2.63450443e-06]\n [2.64806272e-06 9.69405755e-06 6.32611045e-06 1.68283968e-05\n  3.01650334e-06 5.38990134e-03 2.98046461e-06 9.93769348e-01\n  3.33425269e-04 4.65652556e-04]\n [8.02288851e-05 2.77255167e-05 9.88375068e-01 2.20840288e-04\n  3.10605415e-03 1.81256414e-08 8.17760825e-03 5.51495738e-10\n  1.24665394e-05 2.27728725e-09]\n [7.63632078e-03 2.23117135e-03 5.09268701e-01 1.23572186e-01\n  7.83503503e-02 2.57636514e-03 1.74609032e-02 9.84215076e-05\n  2.58751959e-01 5.35918953e-05]\n [3.45268374e-04 3.62465368e-03 2.33240818e-04 9.95570004e-01\n  1.54186142e-04 4.08447995e-08 6.63145693e-05 2.77574759e-06\n  5.70489817e-07 2.91907418e-06]\n [5.01100885e-06 8.75806836e-06 1.98947473e-05 6.34099779e-05\n  1.56454989e-05 6.88719153e-02 8.42789541e-06 9.19192553e-01\n  6.00443501e-03 5.81000419e-03]\n [5.56088006e-03 9.91259873e-01 6.19056576e-04 1.43437006e-03\n  1.01455930e-03 6.71101816e-07 1.02999140e-04 2.95669179e-06\n  4.24115024e-06 3.65196172e-07]\n [2.22552381e-03 5.31223835e-03 9.58871663e-01 2.07665795e-03\n  1.10508678e-02 2.36258231e-04 1.92148052e-02 2.42829788e-06\n  9.95975803e-04 1.37700808e-05]\n [2.26745105e-04 1.72406563e-03 9.90142584e-01 3.04635643e-04\n  4.79411380e-03 5.03302111e-08 2.80197570e-03 1.17683276e-08\n  5.75631657e-06 4.99135879e-08]\n [3.58951002e-06 1.88124337e-04 4.21939902e-02 1.33592303e-05\n  9.55441236e-01 1.62840550e-07 2.08550645e-03 1.88528304e-09\n  7.41305485e-05 1.45573749e-08]\n [6.16652705e-02 1.15578860e-01 2.08408777e-02 7.45159566e-01\n  2.39897389e-02 9.58977442e-04 2.10364833e-02 2.82027735e-03\n  7.37851812e-03 5.71457611e-04]\n [4.40051556e-02 1.36239864e-02 1.11164853e-01 7.89568871e-02\n  1.13173388e-01 7.86206201e-02 5.22805691e-01 2.41210335e-03\n  3.25306803e-02 2.70655425e-03]\n [7.22237819e-05 1.76270601e-06 3.92847322e-03 2.92897137e-04\n  3.95749928e-03 3.89507739e-04 2.76145479e-03 9.24550750e-06\n  9.88570869e-01 1.60424988e-05]\n [2.32279544e-05 8.85769623e-05 2.26068124e-01 6.48475077e-04\n  7.31053174e-01 2.25904611e-08 4.19655778e-02 1.45048151e-09\n  1.52790890e-04 8.78139161e-09]\n [2.06798956e-01 7.50842690e-03 6.60274863e-01 2.49041151e-02\n  1.39774680e-02 9.22009567e-05 7.66413733e-02 5.29573799e-06\n  9.78495181e-03 1.23235804e-05]\n [5.53901494e-01 1.22754398e-04 1.55869536e-02 1.98270567e-02\n  5.52617712e-04 7.68647533e-06 4.06825960e-01 1.49059144e-06\n  3.17130052e-03 2.66220786e-06]\n [7.88552407e-03 3.04065961e-02 2.46369769e-03 9.39321816e-01\n  1.41911153e-02 3.59947990e-06 5.54919662e-03 5.05664502e-05\n  4.65724879e-05 8.12240978e-05]\n [3.36909434e-05 7.31708738e-08 4.37730123e-05 2.62228497e-07\n  3.95645584e-05 6.68302982e-06 2.69698285e-05 9.64663559e-06\n  9.99835253e-01 4.06129766e-06]\n [9.73409158e-04 6.87147331e-05 9.51231599e-01 5.31843863e-04\n  6.57149218e-03 2.00343973e-08 4.04731110e-02 4.18995683e-09\n  1.49747226e-04 6.31101145e-08]\n [1.15847312e-01 1.30765839e-04 8.28254689e-03 6.65992498e-02\n  2.39349040e-03 2.71113868e-06 8.05594325e-01 4.42521105e-06\n  1.14271778e-03 2.49794198e-06]\n [2.39862202e-04 1.14751938e-05 5.81609165e-05 6.53645766e-05\n  2.20763632e-05 3.09246220e-02 1.74958725e-04 2.83271093e-02\n  2.61648255e-03 9.37559903e-01]\n [1.92172080e-02 1.01225087e-02 9.26387403e-03 7.80810237e-01\n  1.46740481e-01 7.06812571e-05 2.86801830e-02 1.56500973e-04\n  4.90552885e-03 3.29412323e-05]\n [8.44593451e-05 1.54023874e-05 1.08439475e-04 2.81890752e-05\n  4.25162398e-05 9.94191706e-01 1.81613199e-04 9.53154115e-04\n  1.40600430e-03 2.98859994e-03]\n [2.13917042e-03 1.89034792e-04 8.11240226e-02 1.83302863e-03\n  3.17585677e-01 3.62788774e-06 5.90323806e-01 8.67386660e-08\n  6.80110045e-03 4.81165785e-07]\n [1.09414451e-01 5.29526034e-04 7.69134820e-01 4.08147555e-03\n  2.76085618e-03 1.52617838e-06 9.11413282e-02 4.49943218e-06\n  2.29291562e-02 2.38870985e-06]\n [1.14396702e-04 2.27469089e-03 5.70093572e-01 1.42343878e-03\n  4.18107510e-01 2.66946557e-07 7.65583990e-03 2.15246239e-08\n  3.30292853e-04 1.43047822e-08]\n [1.98438211e-04 3.01235792e-04 1.58422306e-01 6.24305801e-03\n  8.03845406e-01 6.02621242e-08 3.06171365e-02 5.23877786e-09\n  3.72407114e-04 4.95304553e-09]\n [6.54764066e-04 8.09216523e-04 9.35596108e-01 3.10344796e-04\n  3.40793915e-02 1.10283099e-05 2.83534639e-02 1.39068163e-07\n  1.84316683e-04 1.06420180e-06]\n [2.45198612e-06 7.25575831e-08 4.59718706e-07 9.59937893e-07\n  7.20490823e-09 6.25311071e-03 4.42448822e-07 9.42133134e-04\n  9.44964904e-06 9.92790937e-01]\n [1.12396748e-07 2.45906087e-08 9.70822285e-08 4.07724286e-07\n  5.78365871e-08 1.40724331e-03 5.31907851e-08 8.89755607e-01\n  3.15059675e-04 1.08521327e-01]\n [6.98517542e-03 3.00195243e-04 5.20532310e-04 9.90429044e-01\n  2.54839222e-04 3.37985249e-08 1.49195746e-03 5.50844163e-07\n  1.74843608e-05 3.84917342e-07]\n [7.48020828e-01 9.99165568e-05 1.27482995e-01 2.42780102e-03\n  1.53797300e-04 3.45878415e-09 1.20168366e-01 4.97214714e-08\n  1.64228270e-03 3.79768676e-06]\n [1.97403249e-03 1.74080371e-03 1.01074219e-01 4.32408117e-02\n  7.19566822e-01 2.46224067e-06 1.29872248e-01 2.81815915e-07\n  2.52802437e-03 3.85074372e-07]\n [3.02788658e-06 8.43346243e-06 3.63396466e-05 2.51453930e-05\n  2.20094753e-05 8.51445079e-01 1.13217848e-05 1.45258754e-01\n  1.42614741e-03 1.76365220e-03]\n [3.68114118e-03 1.90345682e-02 3.01515222e-01 1.32625448e-02\n  6.43003345e-01 2.08369929e-05 8.67226254e-03 4.78598076e-06\n  1.08026750e-02 2.75011962e-06]\n [9.58459258e-01 5.23647623e-06 7.84786709e-04 1.38920115e-03\n  1.45575914e-05 2.72440275e-08 3.92449908e-02 1.30759918e-08\n  1.01867023e-04 4.48352964e-08]\n [5.81962595e-05 8.12570914e-04 1.73208579e-01 2.03757198e-04\n  8.13971341e-01 1.13506746e-06 1.06393369e-02 4.90412013e-08\n  1.10483612e-03 8.62412151e-08]\n [9.10762489e-01 2.37881181e-06 9.28912952e-04 6.75672549e-04\n  1.39973563e-05 6.51632470e-10 8.75898004e-02 7.05353109e-10\n  2.67345185e-05 6.02516970e-09]\n [8.81400425e-04 1.08371103e-04 1.17702328e-01 2.30450393e-03\n  1.41724899e-01 1.69565865e-05 7.35965192e-01 3.49175195e-08\n  1.29617658e-03 2.04520830e-07]\n [1.33196590e-04 9.99018908e-01 8.73922836e-05 5.55404113e-04\n  1.91197687e-04 1.12517705e-06 1.51633969e-06 8.61030185e-06\n  9.41964913e-07 1.75961907e-06]\n [1.74578605e-03 4.13747877e-03 5.95780194e-01 4.30323463e-03\n  3.68926704e-01 5.32868435e-05 2.20760256e-02 3.11571898e-06\n  2.97145406e-03 2.78121206e-06]\n [2.31120771e-06 9.99845505e-01 6.09280596e-06 1.18660770e-04\n  2.73401183e-05 1.80307325e-08 1.16940591e-08 1.09555721e-07\n  1.17966161e-08 3.64545620e-08]\n [2.05104861e-05 1.23327900e-05 1.40492193e-05 4.99250382e-05\n  4.73000227e-06 1.43980542e-02 1.30027265e-05 9.17535543e-01\n  5.78666630e-04 6.73732609e-02]\n [1.25392154e-02 6.88233739e-03 5.55706859e-01 2.66101174e-02\n  3.24044973e-01 6.98652220e-05 6.80098385e-02 6.43229487e-06\n  6.12610159e-03 4.22979383e-06]\n [1.69674313e-05 6.52286261e-08 4.41937955e-05 2.72778919e-07\n  5.80791821e-05 1.41084101e-07 1.87236310e-05 4.10517896e-06\n  9.99853969e-01 3.41673376e-06]\n [5.01386130e-05 8.86468024e-06 4.24246937e-05 4.59951225e-05\n  1.07965325e-05 9.96518016e-01 4.21037767e-05 7.39450101e-04\n  2.11019092e-03 4.32099652e-04]\n [4.39693504e-05 9.97669995e-01 5.52842030e-05 1.99777423e-03\n  2.26117671e-04 1.59810853e-07 8.66315361e-07 3.96186715e-06\n  2.11787892e-07 1.66146026e-06]\n [4.76555928e-04 5.21644251e-04 3.51077470e-04 9.96702254e-01\n  1.43000938e-03 4.22180122e-08 5.08454512e-04 4.20179248e-07\n  9.44210024e-06 2.05644994e-07]\n [5.46290517e-01 3.14549319e-02 2.06974611e-01 7.28200525e-02\n  2.17058472e-02 9.11798794e-04 1.04796462e-01 3.26998736e-04\n  1.44681837e-02 2.50566722e-04]\n [1.21976293e-04 6.03487933e-05 6.52643095e-04 4.54197259e-04\n  2.49211123e-04 9.89703715e-01 4.93025756e-04 3.28543433e-03\n  3.36780446e-03 1.61160715e-03]], shape=(96, 10), dtype=float32)\ny_true: (96,)    tf.Tensor(\n[6 0 0 9 2 9 3 0 2 8 1 1 4 2 1 8 9 4 0 8 0 1 3 5 3 8 4 1 9 0 0 3 1 2 1 7 7\n 5 8 0 2 2 1 3 8 8 6 7 2 2 3 5 1 2 2 4 3 6 8 4 2 0 3 8 6 6 9 3 5 6 0 4 4 2\n 9 7 3 2 4 5 4 0 4 6 6 1 4 1 7 2 8 5 1 3 0 5], shape=(96,), dtype=uint8)\nloss outputs: tf.Tensor(\n[1.7711477e+00 1.0870743e+00 1.7860690e-01 3.5490909e-01 3.4324706e-01\n 1.6039857e-01 2.0738648e-01 3.1467552e+00 1.6417643e-01 1.8329139e-01\n 5.3268928e-02 4.8442381e-03 1.6676612e-01 6.5008235e-01 3.9099883e-05\n 7.9029426e-02 4.3813542e-02 5.5469595e-02 9.4482869e-02 6.7556977e-02\n 4.6002872e-02 1.1910257e-03 9.1956162e-01 6.7715295e-02 1.2000443e-01\n 7.5216609e-04 2.8758222e-01 2.4859973e-03 2.5967142e-02 5.5796370e-02\n 3.1366026e-01 4.6475250e-03 1.0620984e-04 2.3674059e-01 4.1483971e-05\n 1.0989076e+00 1.8020120e-03 1.0980066e-02 3.1327567e+00 2.5498701e-02\n 9.0405956e-02 3.3282503e-02 7.6177106e-03 9.3833321e-01 2.8759522e-02\n 9.4882585e-02 2.6576644e-01 6.2501207e-03 1.1693388e-02 6.7477953e-01\n 4.4399733e-03 2.6755068e+00 8.7785516e-03 4.1998062e-02 9.9066496e-03\n 4.5582224e-02 2.9415697e-01 6.4854538e-01 1.1494954e-02 3.1326932e-01\n 4.1509905e-01 5.9076840e-01 6.2597118e-02 1.6473368e-04 3.2071178e+00\n 2.1617505e-01 6.4474642e-02 2.4742316e-01 5.8252094e-03 5.2708411e-01\n 2.2126124e+00 8.7201679e-01 2.1834853e-01 6.6571407e-02 7.2352770e-03\n 1.1680854e-01 9.6171880e-03 2.0597723e+00 3.2910585e-01 1.6082029e-01\n 4.4160536e-01 4.2428561e-02 2.0583011e-01 2.4350910e+00 3.0657253e-01\n 9.8156428e-04 9.9715734e-01 1.5496007e-04 8.6063981e-02 5.8751434e-01\n 1.4602071e-04 3.4880531e-03 2.3327062e-03 3.3033111e-03 6.0460430e-01\n 1.0349652e-02], shape=(96,), dtype=float32)\nepoch 1, loss 0.4510, train acc 0.840, test acc 0.840\n"
    }
   ],
   "source": [
    "# 激活函数\n",
    "with tf.device('CPU:0'):\n",
    "    def relu(x):\n",
    "        return tf.math.maximum(x, 0)\n",
    "\n",
    "    # 模型\n",
    "    def net(X):\n",
    "        X = tf.reshape(X, shape=[-1, num_inputs])\n",
    "        h = relu(tf.matmul(X,W1)+b1)\n",
    "        return tf.math.softmax(tf.matmul(h, W2)+b2)\n",
    "\n",
    "    # 损失函数\n",
    "    # 分类损失函数：\n",
    "    # （1）tf.keras.losses\n",
    "    # CategoricalCrossentropy || categorical_crossentropy 这个用于 one-hot形式的 y_pred\n",
    "    # SparseCategoricalCrossentropy || sparse_categorical_crossentropy 用于  a single floating point value per example for y_true and classes\n",
    "    # （2）tf.nn\n",
    "    # sparse_softmax_cross_entropy_with_logits 这个损失函数内部进行softmax，不要把softmax的输出作为它的输入！\n",
    "\n",
    "    ## eg:\n",
    "    # tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
    "    # tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=losses_utils.ReductionV2.AUTO,name='sparse_categorical_crossentropy')\n",
    "\n",
    "    def entropy_loss(y_hat, y_true):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_hat)\n",
    "\n",
    "    # 训练\n",
    "\n",
    "\n",
    "    def evaluate_accuracy(test_iter, net):\n",
    "        acc_sum, n = 0.0, 0\n",
    "        for x, y in test_iter:\n",
    "            y_pred = net(x)\n",
    "            y_pred = tf.cast(tf.argmax(y_pred, axis=1), dtype=tf.int64)\n",
    "            y = tf.cast(y, dtype=tf.int64)\n",
    "            acc_sum += np.sum(y==y_pred)\n",
    "            n += x.shape[0]\n",
    "        return acc_sum / n\n",
    "\n",
    "\n",
    "\n",
    "    def train_net(net, train_iter, test_iter, loss_function, num_epochs, batch_size, params=None, lr=0.01, optimizer=None):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "            for X, y in train_iter:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = net(X)\n",
    "                    # print('net(x) outputs y:', y_pred.shape, '  ', y_pred)\n",
    "                    # print('y_true:', y.shape, '  ', y)\n",
    "                    # loss_value = loss_function(y_pred, y)\n",
    "                    loss_value = tf.reduce_sum(loss_function(y_pred, y))\n",
    "                    print('loss outputs:', loss_value)\n",
    "                    # break\n",
    "                grads = tape.gradient(loss_value, params)\n",
    "                \n",
    "                if optimizer is None:\n",
    "                    for i, param in enumerate(params):\n",
    "                        param.assign_sub(lr*grads[i] / batch_size)\n",
    "                else:\n",
    "                    optimizer.apply_gradients(zip([grad/batch_size for grad in grads], params))\n",
    "\n",
    "                # evaluate\n",
    "                y = tf.cast(y, dtype=tf.float32)\n",
    "                train_loss_sum += loss_value.numpy()\n",
    "                train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_pred, axis=1)==tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()\n",
    "                n += y.shape[0]\n",
    "\n",
    "            test_acc = evaluate_accuracy(test_iter, net)\n",
    "            print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'%(epoch+1, train_loss_sum / n, train_acc_sum/n, test_acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    num_epochs, lr = 1, 0.1\n",
    "    train_net(net, train_iter, test_iter, entropy_loss, num_epochs, batch_size, [W1, b1, W2, b2], lr, tf.keras.optimizers.SGD(lr\n",
    "    ))\n",
    "\n",
    "    # evaluate_accuracy(test_iter, net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_1 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               200960    \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                2570      \n=================================================================\nTotal params: 203,530\nTrainable params: 203,530\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/5\n60000/60000 [==============================] - 19s 310us/sample - loss: 354274.0927 - accuracy: 0.1002 - val_loss: 2.3256 - val_accuracy: 0.1000\nEpoch 2/5\n60000/60000 [==============================] - 12s 196us/sample - loss: 2.3171 - accuracy: 0.0990 - val_loss: 2.3132 - val_accuracy: 0.1000\nEpoch 3/5\n60000/60000 [==============================] - 13s 215us/sample - loss: 2.3172 - accuracy: 0.0999 - val_loss: 2.3076 - val_accuracy: 0.1000\nEpoch 4/5\n60000/60000 [==============================] - 14s 241us/sample - loss: 2.3165 - accuracy: 0.1005 - val_loss: 2.3135 - val_accuracy: 0.1000\nEpoch 5/5\n60000/60000 [==============================] - 12s 204us/sample - loss: 2.3166 - accuracy: 0.1006 - val_loss: 2.3205 - val_accuracy: 0.1000\n"
    }
   ],
   "source": [
    "with tf.device('CPU:0'):\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(x_trian, y_train, epochs=5, batch_size=8, validation_data=(x_test, y_test), validation_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout\n",
    " \n",
    " def dropout(x, drop_prob):\n",
    "     assert 0<=drop_prob<=1\n",
    "\n",
    "     keep_prob = 1-drop_prob\n",
    "\n",
    "     if keep_prob==0:\n",
    "         return tf.zeros_like(x)\n",
    "        \n",
    "     mask = tf.random.uniform(shape=x.shape, minval=0, maxval=1,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}